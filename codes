# R 
#install.packages("tm")
library(tm)
#postive tokenize
my.dir<-"/home/admin1/Documents/Machine_learning/txt_sentoken/pos"
my.token.list<-list()
files <- list.files(my.dir) 
for(i in 1:length(files)){temp<-readLines(paste(my.dir,"/",files[i],sep=''))
temp.token<-preprocess(temp)
my.token.list[length(my.token.list)+1]<-list('token'=temp.token)
}
#negative tokenize
my.dir<-"/home/admin1/Documents/Machine_learning/txt_sentoken/neg/"
my.token.list.neg<-list()
files <- list.files(my.dir) 
for(i in 1:length(files)){temp<-readLines(paste(my.dir,"/",files[i],sep=''))
temp.token<-preprocess(temp)
my.token.list.neg[length(my.token.list.neg)+1]<-list('token'=temp.token)
}
#unigram preprocess, require tm package
preprocess<-function(x){
  my.data<-x
  data.remove.punct<-removePunctuation(my.data,preserve_intra_word_dashes = TRUE)
  data.remove.punct.num<-gsub('[0-9]+','', data.remove.punct)
  data.split<-strsplit(data.remove.punct.num,' ')
  data.split.unlist<-unlist(data.split)
  data.split.unlist<-data.split.unlist[data.split.unlist != '']
  data.table<-table(data.split.unlist)
  data.table<-data.table/sum(data.table)
  data.table<-sort(data.table, decreasing = TRUE)
  return(data.table)
}
#select first 100 tokens from each file, just for testing the full model
# the strategy is to cut down as much words as possible, so select first 100 tokens from each file
# and then combine together and further to select first 2000 most common words.  
#positive
positive.100<-lapply(my.token.list, function(x) x[1:100])
positive.100.dataframe<-lapply(positive.100, function(x) t(as.data.frame(x)))
#negative
negative.100<-lapply(my.token.list.neg, function(x) x[1:100])
negative.100.dataframe<-lapply(negative.100, function(x) t(as.data.frame(x)))
#generate token list 
colnames.p100<-lapply(positive.100.dataframe, function(x) colnames(x))
colnames.p100<-unlist(colnames.p100)# the length is 99800
colnames.n100<-lapply(negative.100.dataframe, function(x) colnames(x))
colnames.n100<-unlist(colnames.n100) # the length is 99600
#generate first 2000 most common words
p100n100.vocabl<-c(colnames.p100,colnames.n100) #the length is 199400
vocab.2000<-names(head(sort(table(p100n100.vocabl),decreasing = TRUE),2000))
# and save the vacab.2000 as a R file "vocab2000"
fit<-glm(type~.,family = binomial(link = "logit"), data=model.2000)
# I have realized that I should lemmatize the token list. then I might get a better result.
# But how should I do it? and what about bigram?
