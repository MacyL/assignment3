# R 
#install.packages("tm")
library(tm)
#postive tokenize
my.dir<-"/home/admin1/Documents/Machine_learning/txt_sentoken/pos"
my.token.list<-list()
files <- list.files(my.dir) 
for(i in 1:length(files)){temp<-readLines(paste(my.dir,"/",files[i],sep=''))
temp.token<-preprocess(temp)
my.token.list[length(my.token.list)+1]<-list('token'=temp.token)
}
#negative tokenize
my.dir<-"/home/admin1/Documents/Machine_learning/txt_sentoken/neg/"
my.token.list.neg<-list()
files <- list.files(my.dir) 
for(i in 1:length(files)){temp<-readLines(paste(my.dir,"/",files[i],sep=''))
temp.token<-preprocess(temp)
my.token.list.neg[length(my.token.list.neg)+1]<-list('token'=temp.token)
}
#unigram preprocess, require tm package
preprocess<-function(x){
  my.data<-x
  data.remove.punct<-removePunctuation(my.data,preserve_intra_word_dashes = TRUE)
  data.remove.punct.num<-gsub('[0-9]+','', data.remove.punct)
  data.split<-strsplit(data.remove.punct.num,' ')
  data.split.unlist<-unlist(data.split)
  data.split.unlist<-data.split.unlist[data.split.unlist != '']
  data.table<-table(data.split.unlist)
  data.table<-data.table/sum(data.table)
  data.table<-sort(data.table, decreasing = TRUE)
  return(data.table)
}
#select first 100 tokens from each file
#positive
positive.100<-lapply(my.token.list, function(x) x[1:100])
